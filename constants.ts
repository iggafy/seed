import { NodeType } from './types';

export const NODE_COLORS: Record<NodeType, string> = {
  [NodeType.CONCEPT]: '#3b82f6', // Bright Blue
  [NodeType.TECHNOLOGY]: '#10b981', // Emerald
  [NodeType.PROBLEM]: '#ef4444', // Red
  [NodeType.ENTITY]: '#8b5cf6', // Violet
  [NodeType.QUESTION]: '#f59e0b', // Amber
  [NodeType.TRACE]: '#d946ef', // Fuchsia
};

// SVG Paths (based on 24x24 viewbox)
export const NODE_ICONS: Record<NodeType, string> = {
  // Lightbulb (Filled)
  [NodeType.CONCEPT]: "M9 21c0 .55.45 1 1 1h4c.55 0 1-.45 1-1v-1H9v1zm3-19C8.14 2 5 5.14 5 9c0 2.38 1.19 4.47 3 5.74V17c0 .55.45 1 1 1h6c.55 0 1-.45 1-1v-2.26c1.81-1.27 3-3.36 3-5.74 0-3.86-3.14-7-7-7z",

  // Zap (Already Filled)
  [NodeType.TECHNOLOGY]: "M13 2L3 14h9l-1 8 10-12h-9l1-8z",

  // Exclamation Mark (Filled)
  [NodeType.PROBLEM]: "M12 2C11.45 2 11 2.45 11 3V13C11 13.55 11.45 14 12 14C12.55 14 13 13.55 13 13V3C13 2.45 12.55 2 12 2ZM12 16C11.45 16 11 16.45 11 17C11 17.55 11.45 18 12 18C12.55 18 13 17.55 13 17C13 16.45 12.55 16 12 16Z",

  // User (Already Semi-Filled)
  [NodeType.ENTITY]: "M20 21v-2a4 4 0 0 0-4-4H8a4 4 0 0 0-4 4v2 M12 11a4 4 0 1 0 0-8 4 4 0 0 0 0 8",

  // Question Mark (Full Circular Dot + Hook)
  [NodeType.QUESTION]: "M15.07 11.25l-.9.92C13.45 12.9 13 13.5 13 15h-2v-.5c0-1.1.45-2.1 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41 0-1.1-.9-2-2-2s-2 .9-2 2H8c0-2.21 1.79-4 4-4s4 1.79 4 4c0 .88-.36 1.68-.93 2.25z M12 17a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3z",

  // Search (Filled)
  [NodeType.TRACE]: "M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z",
};

export const NOVEL_SEEDS = [
  { label: 'Mechanistic Interpretability', type: NodeType.QUESTION, description: 'How do we understand the internal circuits of a transformer model beyond just output weights?' },
  { label: 'LLM Hallucination Vectors', type: NodeType.PROBLEM, description: 'Detecting and mitigating latent space regions that trigger confident but false information.' },
  { label: 'RAG Context Precision', type: NodeType.QUESTION, description: 'Balancing chunk size and retrieval relevance to provide optimal grounding for agentic reasoning.' },
  { label: 'Multi-Agent Consensus Delay', type: NodeType.PROBLEM, description: 'Latency and divergent logic issues when autonomous AI agents negotiate complex tasks.' },
  { label: 'Synthetic Data Collapse', type: NodeType.PROBLEM, description: 'Degradation of model quality when trained on content generated by previous AI generations.' },
  { label: 'Zero-Shot Reasoning Limits', type: NodeType.QUESTION, description: 'What are the fundamental bounds of a modelâ€™s ability to solve problems it has never seen in training?' },
  { label: 'Neuro-Symbolic Hybridization', type: NodeType.TECHNOLOGY, description: 'Combining deep learning with classic symbolic logic to improve AI reliability and traceability.' },
  { label: 'Vector Quantization Loss', type: NodeType.PROBLEM, description: 'Managing the trade-off between semantic search speed and the accuracy of compressed embeddings.' },
  { label: 'Reward Misspecification', type: NodeType.PROBLEM, description: 'When an AI optimizes for a proxy reward in ways that conflict with its original intended goal.' },
  { label: 'Diffused Knowledge Privacy', type: NodeType.QUESTION, description: 'Can we effectively "erase" specific PII from a trained model without retraining from scratch?' }
];

export const RELATION_OPTIONS = [
  "enables",
  "inhibits",
  "amplifies",
  "mitigates",
  "requires",
  "obsoletes",
  "questions",
  "answers",
  "synergizes with",
  "conflicts with",
  "is analogous to",
  "evolves into"
];

export const INITIAL_DATA = {
  nodes: [],
  links: []
};